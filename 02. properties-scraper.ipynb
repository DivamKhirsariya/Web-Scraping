{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60123d02-33e6-4e36-88d8-589323a11af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The webpage \"India Real Estate Property Site - Buy Sell Rent Properties Portal - 99acres.com\" did get fully loaded.\n",
      "The webpage \"Property in Chennai - Real Estate in Chennai\" did get fully loaded.\n",
      "All filters visible.\n",
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping page 6...\n",
      "Scraping page 7...\n",
      "Scraping page 8...\n",
      "Scraping page 9...\n",
      "Scraping page 10...\n",
      "Scraping page 11...\n",
      "Scraping page 12...\n",
      "Scraping page 13...\n",
      "Scraping page 14...\n",
      "Scraping page 15...\n",
      "Scraping page 16...\n",
      "Scraping page 17...\n",
      "Scraping page 18...\n",
      "Scraping page 19...\n",
      "Scraping page 20...\n",
      "Scraping page 21...\n",
      "Scraping page 22...\n",
      "Scraping page 23...\n",
      "Scraping page 24...\n",
      "Scraping page 25...\n",
      "Scraping page 26...\n",
      "Scraping page 27...\n",
      "Scraping page 28...\n",
      "Scraping page 29...\n",
      "Scraping page 30...\n",
      "Scraping page 31...\n",
      "Scraping page 32...\n",
      "Scraping page 33...\n",
      "Scraping page 34...\n",
      "Scraping page 35...\n",
      "Scraping page 36...\n",
      "Scraping page 37...\n",
      "Scraping page 38...\n",
      "Scraping page 39...\n",
      "Scraping page 40...\n",
      "Scraping page 41...\n",
      "Scraping page 42...\n",
      "Scraping page 43...\n",
      "Scraping page 44...\n",
      "Scraping page 45...\n",
      "Scraping page 46...\n",
      "Scraping page 47...\n",
      "Scraping page 48...\n",
      "Scraping page 49...\n",
      "Scraping page 50...\n",
      "Scraping page 51...\n",
      "Scraping page 52...\n",
      "Reached last page 52. Exiting.\n",
      "Scraping finished.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "\n",
    "def wait_for_page_to_load(driver, wait):\n",
    "    title = driver.title\n",
    "    try:\n",
    "        wait.until(lambda d: d.execute_script(\"return document.readyState\") == \"complete\")\n",
    "    except:\n",
    "        print(f'The webpage \"{title}\" did not get fully loaded.')\n",
    "    else:\n",
    "        print(f'The webpage \"{title}\" did get fully loaded.')\n",
    "\n",
    "\n",
    "# Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-http2\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
    "chrome_options.add_argument(\"--enable-features=NetworkServiceInProcess\")\n",
    "chrome_options.add_argument(\"--disable-features=NetworkService\")\n",
    "chrome_options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"\n",
    ")\n",
    "\n",
    "# Initialize driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.maximize_window()\n",
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "# Go to the website\n",
    "url = \"https://www.99acres.com/\"\n",
    "driver.get(url)\n",
    "wait_for_page_to_load(driver, wait)\n",
    "\n",
    "# Enter search term\n",
    "wait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"keyword2\"]'))).send_keys(\"Chennai\")\n",
    "time.sleep(2)\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"0\"]'))).click()\n",
    "time.sleep(2)\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"searchform_search_btn\"]'))).click()\n",
    "wait_for_page_to_load(driver, wait)\n",
    "\n",
    "# Adjust budget slider\n",
    "slider = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"budgetLeftFilter_max_node\"]')))\n",
    "actions = ActionChains(driver)\n",
    "actions.click_and_hold(slider).move_by_offset(-73, 0).release().perform()\n",
    "time.sleep(2)\n",
    "\n",
    "# Filters\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[3]/span[2]'))).click()\n",
    "time.sleep(1)\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[1]/div[5]/span[2]'))).click()\n",
    "time.sleep(1)\n",
    "\n",
    "# Move to next filters\n",
    "while True:\n",
    "    try:\n",
    "        btn = wait.until(EC.presence_of_element_located((By.XPATH, \"//i[contains(@class,'iconS_Common_24 icon_upArrow cc__rightArrow')]\")))\n",
    "    except:\n",
    "        print(\"All filters visible.\")\n",
    "        break\n",
    "    else:\n",
    "        btn.click()\n",
    "        time.sleep(1)\n",
    "\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[6]/span[2]'))).click()\n",
    "time.sleep(1)\n",
    "wait.until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div[1]/div[1]/div[4]/div[3]/div[1]/div[3]/section[1]/div[1]/div[1]/div[1]/div[1]/div[2]/div[1]/div[7]/span[2]'))).click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Scrape data\n",
    "page_count = 0\n",
    "data = []\n",
    "\n",
    "while True:\n",
    "    page_count += 1\n",
    "    print(f\"Scraping page {page_count}...\")\n",
    "\n",
    "    rows = driver.find_elements(By.CLASS_NAME, \"tupleNew__TupleContent\")\n",
    "    for row in rows:\n",
    "        try:\n",
    "            name = row.find_element(By.CLASS_NAME, \"tupleNew__headingNrera\").text\n",
    "        except:\n",
    "            name = np.nan\n",
    "\n",
    "        try:\n",
    "            location = row.find_element(By.CLASS_NAME, \"tupleNew__propType\").text\n",
    "        except:\n",
    "            location = np.nan\n",
    "\n",
    "        try:\n",
    "            price = row.find_element(By.CLASS_NAME, \"tupleNew__priceValWrap\").text\n",
    "        except:\n",
    "            price = np.nan\n",
    "\n",
    "        try:\n",
    "            elements = row.find_elements(By.CLASS_NAME, \"tupleNew__area1Type\")\n",
    "            area, bhk = [ele.text for ele in elements]\n",
    "        except:\n",
    "            area, bhk = [np.nan, np.nan]\n",
    "\n",
    "        data.append({\"name\": name, \"location\": location, \"price\": price, \"area\": area, \"bhk\": bhk})\n",
    "\n",
    "    # Scroll to bottom to trigger next page button load\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    next_page_xpath = \"//a[normalize-space()='Next Page >']\"\n",
    "    try:\n",
    "        next_page_button = driver.find_element(By.XPATH, next_page_xpath)\n",
    "    except:\n",
    "        print(f\"Reached last page {page_count}. Exiting.\")\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_page_button)\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.element_to_be_clickable((By.XPATH, next_page_xpath))).click()\n",
    "        time.sleep(5)\n",
    "    except:\n",
    "        print(f\"Could not click Next Page at page {page_count}. Exiting.\")\n",
    "        break\n",
    "\n",
    "# Save to Excel\n",
    "pd.DataFrame(data).drop_duplicates().to_excel(\"chennai-properties-99acres.xlsx\", index=False)\n",
    "driver.quit()\n",
    "print(\"Scraping finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb51e179-8357-4655-a7a4-68b6097bb7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web_scraping",
   "language": "python",
   "name": "web_scraping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
